{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e852f92-db5b-4b9e-9960-a1044f077801",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64d8d6-3669-4a06-ac2d-b877407188fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Step 2: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sdv.single_table import CTGANSynthesizer, TVAESynthesizer, GaussianCopulaSynthesizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac609805-a870-4139-997e-be2e46eb8a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    " \n",
    "# Step 3: Define CSV file names\n",
    "lookup_files = {\n",
    "    'PolicyStatus': 'PolicyStatus.csv',\n",
    "    'PolicyType': 'PolicyType.csv',\n",
    "    'CoverageType': 'CoverageType.csv'\n",
    "}\n",
    " \n",
    "main_files = {\n",
    "    'Customer': 'Customer.csv',\n",
    "    'Policy': 'Policy.csv',\n",
    "    'Coverage': 'Coverage.csv',\n",
    "    'Claim': 'Claim.csv',\n",
    "    'Beneficiary': 'Beneficiary.csv'\n",
    "}\n",
    "# Step 4: Load datasets\n",
    "dataframes = {}\n",
    "for name, file in {**lookup_files, **main_files}.items():\n",
    "    dataframes[name] = pd.read_csv(file)\n",
    " \n",
    "# Step 5: Synthesizer setup\n",
    "def generate_synthetic(df, table_name, sample_size, model_type='ctgan'):\n",
    "    if model_type == 'ctgan':\n",
    "        model = CTGANSynthesizer()\n",
    "    elif model_type == 'tvae':\n",
    "        model = TVAESynthesizer()\n",
    "    elif model_type == 'copula':\n",
    "        model = GaussianCopulaSynthesizer()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    model.fit(df)  # Corrected line\n",
    "    return model.sample(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03717d6b-b85f-440d-8bb1-abe1909d685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = {\n",
    "#     'Customer': 1000,\n",
    "#     'Policy': 1500,\n",
    "#     'Coverage': 1500,\n",
    "#     'Claim': 800,\n",
    "#     'Beneficiary': 700\n",
    "# }\n",
    " \n",
    "# model_choices = {\n",
    "#     'Customer': 'tvae',\n",
    "#     'Policy': 'copula',\n",
    "#     'Coverage': 'ctgan',\n",
    "#     'Claim': 'ctgan',\n",
    "#     'Beneficiary': 'tvae'\n",
    "# }\n",
    " \n",
    "# synthetic_data = {}\n",
    " \n",
    "# # Generate Customer (independent)\n",
    "# synthetic_data['Customer'] = generate_synthetic(dataframes['Customer'], 'Customer', samples['Customer'], model_choices['Customer'])\n",
    " \n",
    "# # Add customerId if not generated\n",
    "# if 'customerId' not in synthetic_data['Customer'].columns:\n",
    "#     synthetic_data['Customer']['customerId'] = range(1, len(synthetic_data['Customer']) + 1)\n",
    " \n",
    "# # Generate Policy\n",
    "# policy_input = dataframes['Policy'].copy()\n",
    "# policy_input['customerId'] = np.random.choice(synthetic_data['Customer']['customerId'], size=len(policy_input))\n",
    "# policy_input['status_id'] = np.random.choice(dataframes['PolicyStatus']['status_id'], size=len(policy_input))\n",
    "# policy_input['type_id'] = np.random.choice(dataframes['PolicyType']['type_id'], size=len(policy_input))\n",
    "# synthetic_data['Policy'] = generate_synthetic(policy_input, 'Policy', samples['Policy'], model_choices['Policy'])\n",
    " \n",
    "# # Add policyId if not present\n",
    "# if 'policyId' not in synthetic_data['Policy'].columns:\n",
    "#     synthetic_data['Policy']['policyId'] = range(1, len(synthetic_data['Policy']) + 1)\n",
    " \n",
    "# # Generate Coverage\n",
    "# coverage_input = dataframes['Coverage'].copy()\n",
    "# coverage_input['policyId'] = np.random.choice(synthetic_data['Policy']['policyId'], size=len(coverage_input))\n",
    "# coverage_input['coverage_type_id'] = np.random.choice(dataframes['CoverageType']['coverage_id'], size=len(coverage_input))\n",
    "# synthetic_data['Coverage'] = generate_synthetic(coverage_input, 'Coverage', samples['Coverage'], model_choices['Coverage'])\n",
    " \n",
    "# # Generate Claim\n",
    "# claim_input = dataframes['Claim'].copy()\n",
    "# claim_input['policyId'] = np.random.choice(synthetic_data['Policy']['policyId'], size=len(claim_input))\n",
    "# synthetic_data['Claim'] = generate_synthetic(claim_input, 'Claim', samples['Claim'], model_choices['Claim'])\n",
    " \n",
    "# # Generate Beneficiary\n",
    "# beneficiary_input = dataframes['Beneficiary'].copy()\n",
    "# beneficiary_input['policyId'] = np.random.choice(synthetic_data['Policy']['policyId'], size=len(beneficiary_input))\n",
    "# synthetic_data['Beneficiary'] = generate_synthetic(beneficiary_input, 'Beneficiary', samples['Beneficiary'], model_choices['Beneficiary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffcb9a-cdaa-49ef-9ead-310fd9c85159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sdv==1.10.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f663c-93f9-4ea0-b627-a116283aa088",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_files = {\n",
    "    'PolicyStatus': 'PolicyStatus.csv',\n",
    "    'PolicyType': 'PolicyType.csv',\n",
    "    'CoverageType': 'CoverageType.csv',\n",
    "    'PaymentMethod': 'PaymentMethod.csv'\n",
    "}\n",
    "\n",
    "main_files = {\n",
    "    'Customer': 'Customer.csv',\n",
    "    'Policy': 'Policy.csv',\n",
    "    'Coverage': 'Coverage.csv',\n",
    "    'Premium': 'Premium.csv',\n",
    "    'Claim': 'Claim.csv',\n",
    "    'Beneficiary': 'Beneficiary.csv'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9966a-f10f-49bf-a93e-5875ab968a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {}\n",
    "for name, file in {**lookup_files, **main_files}.items():\n",
    "    data[name] = pd.read_csv(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83d3c9-563e-4665-bac4-7d2ce2e35e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdv\n",
    "print(\"SDV version:\", sdv.__version__)\n",
    "\n",
    "from sdv.metadata.multi_table import MultiTableMetadata\n",
    "print(\"MultiTableMetadata add_table:\", MultiTableMetadata.add_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d43614-5be5-4293-9c5b-6f296918eb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.metadata.multi_table import MultiTableMetadata\n",
    "\n",
    "metadata = MultiTableMetadata()\n",
    "\n",
    "# data is your dict of table_name: dataframe\n",
    "metadata.detect_from_dataframes(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808b612-0529-4420-be79-51862c1b976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(metadata.add_table))\n",
    "print(dir(metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83aecbc-8afc-400b-b547-458550a1d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sdv.multi_table import HMASynthesizer\n",
    "\n",
    "synthesizer = HMASynthesizer(metadata)\n",
    "synthesizer.fit(data)\n",
    "\n",
    "synthetic_data = synthesizer.sample()\n",
    "\n",
    "# Save output\n",
    "for table_name, df in synthetic_data.items():\n",
    "    df.to_csv(f'SYN_{table_name}.csv', index=False)\n",
    "    print(f'Saved SYN_{table_name}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508048e-367d-401c-9ee2-533f1b6ca127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Relational GAN for Synthetic Data Generation in Jupyter Notebook\n",
    "# # Author: Nidya & ChatGPT | Goal: Generate synthetic data for main tables using GAN, preserving referential integrity\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "# from collections import defaultdict\n",
    "# import os\n",
    "# import random\n",
    "\n",
    "# # Set seed for reproducibility\n",
    "# torch.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "# random.seed(42)\n",
    "\n",
    "# # ========== CONFIGURATION ==========\n",
    "# main_tables_config = {\n",
    "#     'Customer.csv': 300,\n",
    "#     'Policy.csv': 500,\n",
    "#     'Coverage.csv': 400,\n",
    "#     'Premium.csv': 350,\n",
    "#     'Claim.csv': 450,\n",
    "#     'Beneficiary.csv': 300\n",
    "# }\n",
    "# lookup_tables = {\n",
    "#     'PolicyType': pd.read_csv('PolicyType.csv'),\n",
    "#     'PolicyStatus': pd.read_csv('PolicyStatus.csv'),\n",
    "#     'CoverageType': pd.read_csv('CoverageType.csv'),\n",
    "#     'PaymentMethod': pd.read_csv('PaymentMethod.csv')\n",
    "# }\n",
    "\n",
    "# # ========== UTILITY CLASSES ==========\n",
    "# class SimpleGAN(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim=128):\n",
    "#         super().__init__()\n",
    "#         self.generator = nn.Sequential(\n",
    "#             nn.Linear(16, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, input_dim)\n",
    "#         )\n",
    "#         self.discriminator = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "#             nn.Linear(hidden_dim, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def generate(self, z):\n",
    "#         return self.generator(z)\n",
    "\n",
    "#     def discriminate(self, x):\n",
    "#         return self.discriminator(x)\n",
    "\n",
    "# # ========== FUNCTION TO PROCESS AND TRAIN ==========\n",
    "# def process_and_generate(table_path, table_name, synthetic_rows):\n",
    "#     print(f\"\\nProcessing {table_name}...\")\n",
    "#     df = pd.read_csv(table_path).dropna().reset_index(drop=True)\n",
    "\n",
    "#     encoders = {}\n",
    "#     scalers = {}\n",
    "#     encoded_df = pd.DataFrame()\n",
    "\n",
    "#     # Encode categorical and scale numeric\n",
    "#     for col in df.columns:\n",
    "#         if df[col].dtype == 'object':\n",
    "#             le = LabelEncoder()\n",
    "#             encoded_df[col] = le.fit_transform(df[col])\n",
    "#             encoders[col] = le\n",
    "#         else:\n",
    "#             scaler = MinMaxScaler()\n",
    "#             encoded_df[col] = scaler.fit_transform(df[[col]])\n",
    "#             scalers[col] = scaler\n",
    "\n",
    "#     X = torch.tensor(encoded_df.values, dtype=torch.float32)\n",
    "\n",
    "#     gan = SimpleGAN(input_dim=X.shape[1])\n",
    "#     optimizer_G = optim.Adam(gan.generator.parameters(), lr=0.001)\n",
    "#     optimizer_D = optim.Adam(gan.discriminator.parameters(), lr=0.001)\n",
    "#     criterion = nn.BCELoss()\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(500):\n",
    "#         z = torch.randn(X.shape[0], 16)\n",
    "#         fake = gan.generate(z)\n",
    "\n",
    "#         real_labels = torch.ones(X.shape[0], 1)\n",
    "#         fake_labels = torch.zeros(X.shape[0], 1)\n",
    "\n",
    "#         # Train Discriminator\n",
    "#         optimizer_D.zero_grad()\n",
    "#         d_loss_real = criterion(gan.discriminate(X), real_labels)\n",
    "#         d_loss_fake = criterion(gan.discriminate(fake.detach()), fake_labels)\n",
    "#         d_loss = d_loss_real + d_loss_fake\n",
    "#         d_loss.backward()\n",
    "#         optimizer_D.step()\n",
    "\n",
    "#         # Train Generator\n",
    "#         optimizer_G.zero_grad()\n",
    "#         g_loss = criterion(gan.discriminate(fake), real_labels)\n",
    "#         g_loss.backward()\n",
    "#         optimizer_G.step()\n",
    "\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}: D_loss={d_loss.item():.4f}, G_loss={g_loss.item():.4f}\")\n",
    "\n",
    "#     # Generate synthetic data\n",
    "#     z = torch.randn(synthetic_rows, 16)\n",
    "#     synthetic_data = gan.generate(z).detach().numpy()\n",
    "#     synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "#     # Inverse transform\n",
    "#     for col in synthetic_df.columns:\n",
    "#         if col in encoders:\n",
    "#             synthetic_df[col] = synthetic_df[col].rank(pct=True)\n",
    "#             synthetic_df[col] = (synthetic_df[col] * (len(encoders[col].classes_) - 1)).astype(int)\n",
    "#             synthetic_df[col] = synthetic_df[col].clip(0, len(encoders[col].classes_) - 1)\n",
    "#             synthetic_df[col] = encoders[col].inverse_transform(synthetic_df[col])\n",
    "#         else:\n",
    "#             synthetic_df[col] = scalers[col].inverse_transform(synthetic_df[[col]])\n",
    "\n",
    "#     # Fix FK columns with lookup values\n",
    "#     for lookup_name, lookup_df in lookup_tables.items():\n",
    "#         for col in synthetic_df.columns:\n",
    "#             if lookup_name.lower() in col.lower():\n",
    "#                 values = lookup_df.iloc[:, 0].unique()\n",
    "#                 synthetic_df[col] = np.random.choice(values, size=synthetic_df.shape[0])\n",
    "\n",
    "#     output_path = f\"synthetic_{table_name}.csv\"\n",
    "#     synthetic_df.to_csv(output_path, index=False)\n",
    "#     print(f\"Saved synthetic data: {output_path}\")\n",
    "\n",
    "# # ========== MAIN EXECUTION LOOP ==========\n",
    "# for table_file, rows in main_tables_config.items():\n",
    "#     table_name = table_file.replace('.csv', '')\n",
    "#     process_and_generate(table_file, table_name, rows)\n",
    "\n",
    "# print(\"\\nâœ… Synthetic data generation complete for all main tables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169eb9bf-d3dd-4b7a-b076-64ad4312c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relational GAN for Synthetic Data Generation in Jupyter Notebook\n",
    "# Author: Nidya & ChatGPT | Goal: Generate synthetic data for main tables using GAN, preserving referential integrity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "main_tables_config = {\n",
    "    'Customer.csv': 300,\n",
    "    'Policy.csv': 500,\n",
    "    'Coverage.csv': 400,\n",
    "    'Premium.csv': 350,\n",
    "    'Claim.csv': 450,\n",
    "    'Beneficiary.csv': 300\n",
    "}\n",
    "lookup_tables = {\n",
    "    'PolicyType': pd.read_csv('PolicyType.csv'),\n",
    "    'PolicyStatus': pd.read_csv('PolicyStatus.csv'),\n",
    "    'CoverageType': pd.read_csv('CoverageType.csv'),\n",
    "    'PaymentMethod': pd.read_csv('PaymentMethod.csv')\n",
    "}\n",
    "lookup_values = {key.lower(): df.iloc[:, 0].unique().tolist() for key, df in lookup_tables.items()}\n",
    "\n",
    "# ========== UTILITY CLASSES ==========\n",
    "class SimpleGAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(16, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def generate(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def discriminate(self, x):\n",
    "        return self.discriminator(x)\n",
    "\n",
    "# ========== FUNCTION TO PROCESS AND TRAIN ==========\n",
    "def process_and_generate(table_path, table_name, synthetic_rows):\n",
    "    print(f\"\\nProcessing {table_name}...\")\n",
    "    df = pd.read_csv(table_path).dropna().reset_index(drop=True)\n",
    "\n",
    "    encoders = {}\n",
    "    scalers = {}\n",
    "    encoded_df = pd.DataFrame()\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            encoded_df[col] = le.fit_transform(df[col])\n",
    "            encoders[col] = le\n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "            encoded_df[col] = scaler.fit_transform(df[[col]]).ravel()  # <== FIXED HERE\n",
    "            scalers[col] = scaler\n",
    "\n",
    "\n",
    "    X = torch.tensor(encoded_df.values, dtype=torch.float32)\n",
    "\n",
    "    gan = SimpleGAN(input_dim=X.shape[1])\n",
    "    optimizer_G = optim.Adam(gan.generator.parameters(), lr=0.001)\n",
    "    optimizer_D = optim.Adam(gan.discriminator.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(500):\n",
    "        z = torch.randn(X.shape[0], 16)\n",
    "        fake = gan.generate(z)\n",
    "\n",
    "        real_labels = torch.ones(X.shape[0], 1)\n",
    "        fake_labels = torch.zeros(X.shape[0], 1)\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss_real = criterion(gan.discriminate(X), real_labels)\n",
    "        d_loss_fake = criterion(gan.discriminate(fake.detach()), fake_labels)\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = criterion(gan.discriminate(fake), real_labels)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: D_loss={d_loss.item():.4f}, G_loss={g_loss.item():.4f}\")\n",
    "\n",
    "    # Generate synthetic data\n",
    "    z = torch.randn(synthetic_rows, 16)\n",
    "    synthetic_data = gan.generate(z).detach().numpy()\n",
    "    synthetic_df = pd.DataFrame(synthetic_data, columns=df.columns)\n",
    "\n",
    "    # Inverse transform\n",
    "    for col in synthetic_df.columns:\n",
    "        if col in encoders:\n",
    "            synthetic_df[col] = synthetic_df[col].rank(pct=True)\n",
    "            synthetic_df[col] = (synthetic_df[col] * (len(encoders[col].classes_) - 1)).astype(int)\n",
    "            synthetic_df[col] = synthetic_df[col].clip(0, len(encoders[col].classes_) - 1)\n",
    "            synthetic_df[col] = encoders[col].inverse_transform(synthetic_df[col])\n",
    "        else:\n",
    "            synthetic_df[col] = scalers[col].inverse_transform(synthetic_df[[col]])\n",
    "\n",
    "    # Fix FK columns with lookup values (preserve referential integrity)\n",
    "    for col in synthetic_df.columns:\n",
    "        col_lower = col.lower()\n",
    "        for lookup_key, valid_values in lookup_values.items():\n",
    "            if lookup_key in col_lower:\n",
    "                synthetic_df[col] = np.random.choice(valid_values, size=len(synthetic_df))\n",
    "\n",
    "    output_path = f\"synthetic_{table_name}.csv\"\n",
    "    synthetic_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved synthetic data: {output_path}\")\n",
    "\n",
    "# ========== MAIN EXECUTION LOOP ==========\n",
    "for table_file, rows in main_tables_config.items():\n",
    "    table_name = table_file.replace('.csv', '')\n",
    "    process_and_generate(table_file, table_name, rows)\n",
    "\n",
    "print(\"\\nâœ… Synthetic data generation complete for all main tables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6479d87-e0df-48b4-913c-ff5dfd05071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade sdv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f378b4-e72e-4b88-b1b4-d6a95e093eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f563b5-6023-44f9-9019-c2d8f06c7fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y sdv\n",
    "!{sys.executable} -m pip install sdv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b90b3e-8549-4102-a49f-3ea1ada7e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip show sdv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d548e-bc91-44d4-9a8e-b8e74db05ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SDV if not installed\n",
    "# !pip install sdv pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sdv.tabular import CTGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d80ad-83c4-468d-a8c7-48df2a8efb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.tabular import CTGAN\n",
    "print(\"CTGAN imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee11d4e-6184-4300-ab59-0390ab885e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sdv --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7a169-e74e-46b6-8206-16a147feacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Relational Synthetic Data Generation using SDV HMA1 (Hierarchical Multi-table Architecture)\n",
    "# Author: Nidya & ChatGPT\n",
    "\n",
    "from sdv.multi_table import HMASynthesizer\n",
    "from sdv.metadata import MultiTableMetadata\n",
    "import pandas as pd\n",
    "\n",
    "# ========== LOAD DATA ==========\n",
    "# Please ensure these CSV files are in the current directory\n",
    "main_tables = {\n",
    "    'Customer': pd.read_csv('Customer.csv'),\n",
    "    'Policy': pd.read_csv('Policy.csv'),\n",
    "    'Coverage': pd.read_csv('Coverage.csv'),\n",
    "    'Premium': pd.read_csv('Premium.csv'),\n",
    "    'Claim': pd.read_csv('Claim.csv'),\n",
    "    'Beneficiary': pd.read_csv('Beneficiary.csv')\n",
    "}\n",
    "\n",
    "lookup_tables = {\n",
    "    'PolicyStatus': pd.read_csv('PolicyStatus.csv'),\n",
    "    'PolicyType': pd.read_csv('PolicyType.csv'),\n",
    "    'CoverageType': pd.read_csv('CoverageType.csv'),\n",
    "    'PaymentMethod': pd.read_csv('PaymentMethod.csv')\n",
    "}\n",
    "\n",
    "all_tables = {**main_tables, **lookup_tables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c85f7-2078-4063-ae8d-45a6557a7945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82ff879-6548-47cd-b8c7-a47ea3ea1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ========== DEFINE METADATA ==========\n",
    "metadata = MultiTableMetadata()\n",
    "\n",
    "metadata.add_table(\n",
    "    name='Customer',\n",
    "    data=main_tables['Customer'],\n",
    "    primary_key='customer_id'\n",
    ")\n",
    "\n",
    "metadata.add_table(\n",
    "    name='Policy',\n",
    "    data=main_tables['Policy'],\n",
    "    primary_key='policy_id',\n",
    "    parent='Customer',\n",
    "    foreign_key='customer_id'\n",
    ")\n",
    "\n",
    "metadata.add_table(\n",
    "    name='Claim',\n",
    "    data=main_tables['Claim'],\n",
    "    primary_key='claim_id',\n",
    "    parent='Policy',\n",
    "    foreign_key='policy_id'\n",
    ")\n",
    "\n",
    "metadata.add_table(\n",
    "    name='Beneficiary',\n",
    "    data=main_tables['Beneficiary'],\n",
    "    primary_key='beneficiary_id',\n",
    "    parent='Claim',\n",
    "    foreign_key='claim_id'\n",
    ")\n",
    "\n",
    "metadata.add_table(\n",
    "    name='Coverage',\n",
    "    data=main_tables['Coverage'],\n",
    "    primary_key='coverage_id',\n",
    "    parent='Policy',\n",
    "    foreign_key='policy_id'\n",
    ")\n",
    "\n",
    "metadata.add_table(\n",
    "    name='Premium',\n",
    "    data=main_tables['Premium'],\n",
    "    primary_key='premium_id',\n",
    "    parent='Policy',\n",
    "    foreign_key='policy_id'\n",
    ")\n",
    "\n",
    "# Lookup Tables\n",
    "metadata.add_table(\n",
    "    name='PolicyStatus',\n",
    "    data=lookup_tables['PolicyStatus'],\n",
    "    primary_key='status_id'\n",
    ")\n",
    "metadata.add_table(\n",
    "    name='PolicyType',\n",
    "    data=lookup_tables['PolicyType'],\n",
    "    primary_key='type_id'\n",
    ")\n",
    "metadata.add_table(\n",
    "    name='CoverageType',\n",
    "    data=lookup_tables['CoverageType'],\n",
    "    primary_key='coverage_type_id'\n",
    ")\n",
    "metadata.add_table(\n",
    "    name='PaymentMethod',\n",
    "    data=lookup_tables['PaymentMethod'],\n",
    "    primary_key='method_id'\n",
    ")\n",
    "\n",
    "# Define foreign keys to lookup tables\n",
    "metadata.set_foreign_key('Policy', 'status_id', 'PolicyStatus')\n",
    "metadata.set_foreign_key('Policy', 'type_id', 'PolicyType')\n",
    "metadata.set_foreign_key('Coverage', 'coverage_type_id', 'CoverageType')\n",
    "metadata.set_foreign_key('Premium', 'payment_method_id', 'PaymentMethod')\n",
    "\n",
    "# ========== FIT MODEL ==========\n",
    "print(\"Fitting HMA1 model on relational data...\")\n",
    "model =HMASynthesize(metadata)\n",
    "model.fit(all_tables)\n",
    "print(\"âœ… Model training complete!\")\n",
    "\n",
    "# ========== SYNTHETIC ROW COUNTS ==========\n",
    "sample_sizes = {\n",
    "    'Customer': 100000,\n",
    "    'Policy': 200000,\n",
    "    'Claim': 350000,\n",
    "    'Beneficiary': 300000,\n",
    "    'Coverage': 350000,\n",
    "    'Premium': 200000\n",
    "}\n",
    "\n",
    "# ========== GENERATE SYNTHETIC DATA ==========\n",
    "synthetic_data = model.sample(scale=1.0, max_rows=sample_sizes)\n",
    "\n",
    "# ========== SAVE SYNTHETIC OUTPUT ==========\n",
    "os.makedirs(\"synthetic_output\", exist_ok=True)\n",
    "for table_name, df in synthetic_data.items():\n",
    "    df.to_csv(f\"synthetic_output/{table_name}.csv\", index=False)\n",
    "    print(f\"âœ… Saved: synthetic_output/{table_name}.csv\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All synthetic tables generated with referential integrity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4b4f8a-3e20-46d2-b66f-07156f4289bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sdv.multi_table import HMASynthesizer\n",
    "from sdv.metadata.multi_table import MultiTableMetadata\n",
    "\n",
    "# ======== Load Main Tables ========\n",
    "main_tables = {\n",
    "    'Customer': pd.read_csv('Customer.csv'),\n",
    "    'Policy': pd.read_csv('Policy.csv'),\n",
    "    'Claim': pd.read_csv('Claim.csv'),\n",
    "    'Coverage': pd.read_csv('Coverage.csv'),\n",
    "    'Premium': pd.read_csv('Premium.csv'),\n",
    "    'Beneficiary': pd.read_csv('Beneficiary.csv')\n",
    "}\n",
    "\n",
    "# ======== Define Metadata ========\n",
    "\n",
    "metadata = MultiTableMetadata()\n",
    "\n",
    "# Add tables with just name and data\n",
    "metadata.add_table('Customer', main_tables['Customer'])\n",
    "metadata.add_table('Policy', main_tables['Policy'])\n",
    "metadata.add_table('Claim', main_tables['Claim'])\n",
    "metadata.add_table('Coverage', main_tables['Coverage'])\n",
    "metadata.add_table('Premium', main_tables['Premium'])\n",
    "metadata.add_table('Beneficiary', main_tables['Beneficiary'])\n",
    "\n",
    "# Now manually define the metadata details for each table\n",
    "metadata.tables['Customer']['primary_key'] = 'customer_id'\n",
    "\n",
    "metadata.tables['Policy']['primary_key'] = 'policy_id'\n",
    "metadata.tables['Policy']['fields'] = {\n",
    "    'customer_id': {\n",
    "        'ref': {\n",
    "            'table': 'Customer',\n",
    "            'field': 'customer_id'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata.tables['Claim']['primary_key'] = 'claim_id'\n",
    "metadata.tables['Claim']['fields'] = {\n",
    "    'policy_id': {\n",
    "        'ref': {\n",
    "            'table': 'Policy',\n",
    "            'field': 'policy_id'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata.tables['Coverage']['primary_key'] = 'coverage_id'\n",
    "metadata.tables['Coverage']['fields'] = {\n",
    "    'policy_id': {\n",
    "        'ref': {\n",
    "            'table': 'Policy',\n",
    "            'field': 'policy_id'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata.tables['Premium']['primary_key'] = 'premium_id'\n",
    "metadata.tables['Premium']['fields'] = {\n",
    "    'policy_id': {\n",
    "        'ref': {\n",
    "            'table': 'Policy',\n",
    "            'field': 'policy_id'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata.tables['Beneficiary']['primary_key'] = 'beneficiary_id'\n",
    "metadata.tables['Beneficiary']['fields'] = {\n",
    "    'policy_id': {\n",
    "        'ref': {\n",
    "            'table': 'Policy',\n",
    "            'field': 'policy_id'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Validate metadata\n",
    "metadata.validate(main_tables)\n",
    "\n",
    "\n",
    "\n",
    "# ======== Fit the HMASynthesizer ========\n",
    "synthesizer = HMASynthesizer(metadata)\n",
    "synthesizer.fit(main_tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a538b25-44cf-4ed8-b4e0-6e8dee2865ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== Generate Oversized Sample ========\n",
    "synthetic_data = synthesizer.sample(scale=10)  # scale > 1 to ensure sufficient rows\n",
    "\n",
    "# ======== Trim to desired counts ========\n",
    "desired_counts = {\n",
    "    'Customer': 500,\n",
    "    'Policy': 1200,\n",
    "    'Claim': 2000,\n",
    "    'Coverage': 1800,\n",
    "    'Premium': 1600,\n",
    "    'Beneficiary': 800\n",
    "}\n",
    "\n",
    "final_output = {}\n",
    "\n",
    "for table_name, df in synthetic_data.items():\n",
    "    count = desired_counts.get(table_name, len(df))\n",
    "    final_output[table_name] = df.sample(n=min(count, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ======== Save final tables ========\n",
    "for table, df in final_output.items():\n",
    "    df.to_csv(f'synthetic_{table}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82546a4-1c47-4c76-b8d8-c33992f540d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df720e-54ab-4aae-bed3-e5aa6a7d8c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff674fd-d6d5-4859-9db5-175e2cd966d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f626291-6840-43cb-956a-fbeb61255385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7246de-668e-4d7f-b108-0071f7f5a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Install required library (run this cell once)\n",
    "!pip install -q sdv ipywidgets\n",
    "\n",
    "# STEP 1: Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81cf1ab-d119-48d8-a980-0d65ab880889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Optional: For uploading files via widget in Jupyter (uncomment if you want)\n",
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "#\n",
    "# upload_widget = widgets.FileUpload(accept='.csv', multiple=True)\n",
    "# display(upload_widget)\n",
    "#\n",
    "# # After uploading files, you can access them via upload_widget.value\n",
    "# # But manual loading from disk is simpler for multiple files\n",
    "\n",
    "# STEP 2: Load CSV files manually from current folder\n",
    "# Place your CSV files in the same directory as this notebook or specify the path\n",
    "\n",
    "csv_files = [\n",
    "    'Customer.csv',\n",
    "    'Policy.csv',\n",
    "    'Coverage.csv',\n",
    "    'Premium.csv',\n",
    "    'Claim.csv',\n",
    "    'Beneficiary.csv',\n",
    "    'PolicyStatus.csv',\n",
    "    'PolicyType.csv',\n",
    "    'CoverageType.csv',\n",
    "    'PaymentMethod.csv'\n",
    "]\n",
    "\n",
    "dataframes = {}\n",
    "for filename in csv_files:\n",
    "    try:\n",
    "        df_name = filename.replace('.csv', '')\n",
    "        dataframes[df_name] = pd.read_csv(filename)\n",
    "        print(f\"Loaded {filename} with columns: {dataframes[df_name].columns.tolist()}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Please make sure it's in the notebook folder.\")\n",
    "\n",
    "# STEP 3: Define synthetic sample sizes\n",
    "samples = {\n",
    "    'Customer': 100,\n",
    "    'Policy': 200,\n",
    "    'Coverage': 200,\n",
    "    'Premium': 150,\n",
    "    'Claim': 400,\n",
    "    'Beneficiary': 300\n",
    "}\n",
    "\n",
    "# STEP 4: Reusable CTGAN function\n",
    "def generate_ctgan(data, table_name, num_rows):\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data)\n",
    "    model = CTGANSynthesizer(metadata)\n",
    "    model.fit(data)\n",
    "    synthetic = model.sample(num_rows=num_rows)\n",
    "    print(f\"âœ… Generated synthetic data for {table_name}\")\n",
    "    return synthetic\n",
    "\n",
    "# STEP 5: Generate synthetic data\n",
    "synthetic_data = {}\n",
    "\n",
    "# Customer (independent)\n",
    "synthetic_data['Customer'] = generate_ctgan(dataframes['Customer'], 'Customer', samples['Customer'])\n",
    "\n",
    "# Policy\n",
    "policy_input = dataframes['Policy'].copy()\n",
    "policy_input['customer_id'] = np.random.choice(synthetic_data['Customer']['customer_id'], size=len(policy_input))\n",
    "policy_input['status_id'] = np.random.choice(dataframes['PolicyStatus']['status_id'], size=len(policy_input))\n",
    "policy_input['type_id'] = np.random.choice(dataframes['PolicyType']['type_id'], size=len(policy_input))\n",
    "synthetic_data['Policy'] = generate_ctgan(policy_input, 'Policy', samples['Policy'])\n",
    "\n",
    "# Coverage\n",
    "coverage_input = dataframes['Coverage'].copy()\n",
    "coverage_input['policy_id'] = np.random.choice(synthetic_data['Policy']['policy_id'], size=len(coverage_input))\n",
    "coverage_input['coverage_type_id'] = np.random.choice(dataframes['CoverageType']['coverage_id'], size=len(coverage_input))\n",
    "synthetic_data['Coverage'] = generate_ctgan(coverage_input, 'Coverage', samples['Coverage'])\n",
    "\n",
    "# Premium\n",
    "premium_input = dataframes['Premium'].copy()\n",
    "premium_input['policy_id'] = np.random.choice(synthetic_data['Policy']['policy_id'], size=len(premium_input))\n",
    "premium_input['payment_method_id'] = np.random.choice(dataframes['PaymentMethod']['method_id'], size=len(premium_input))\n",
    "synthetic_data['Premium'] = generate_ctgan(premium_input, 'Premium', samples['Premium'])\n",
    "\n",
    "# Claim\n",
    "claim_input = dataframes['Claim'].copy()\n",
    "claim_input['policy_id'] = np.random.choice(synthetic_data['Policy']['policy_id'], size=len(claim_input))\n",
    "synthetic_data['Claim'] = generate_ctgan(claim_input, 'Claim', samples['Claim'])\n",
    "\n",
    "# Beneficiary\n",
    "beneficiary_input = dataframes['Beneficiary'].copy()\n",
    "beneficiary_input['policy_id'] = np.random.choice(synthetic_data['Policy']['policy_id'], size=len(beneficiary_input))\n",
    "synthetic_data['Beneficiary'] = generate_ctgan(beneficiary_input, 'Beneficiary', samples['Beneficiary'])\n",
    "\n",
    "# STEP 6: Save synthetic CSV files locally\n",
    "for table_name, df in synthetic_data.items():\n",
    "    filename = f\"{table_name}_synthetic.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved synthetic data to {filename}\")\n",
    "\n",
    "# Now you can download these CSVs from the Jupyter file browser or open them directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29a0ec-a7a3-43b1-ac67-51958768dee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e05d5-a93e-49d0-8242-3d5f984c8033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d990247-d36c-4c28-8888-4c92d8ec4379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c795e7-4dfe-44ed-85c6-a80cd7cfefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "import joblib  # for saving models\n",
    "\n",
    "# Load your dataframes here as before (omitted for brevity)\n",
    "\n",
    "# Define a function to train and save a model\n",
    "def train_and_save_ctgan(data, table_name, model_path):\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data)\n",
    "    model = CTGANSynthesizer(metadata)\n",
    "    print(f\"Training model for {table_name} on {len(data)} rows...\")\n",
    "    model.fit(data)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved model for {table_name} at {model_path}\")\n",
    "\n",
    "# Train & save models for all tables (example for Customer and Policy)\n",
    "train_and_save_ctgan(dataframes['Customer'], 'Customer', 'Customer_ctgan.pkl')\n",
    "\n",
    "# For dependent tables, adjust foreign keys before training if needed\n",
    "policy_input = dataframes['Policy'].copy()\n",
    "policy_input['customer_id'] = np.random.choice(dataframes['Customer']['customer_id'], size=len(policy_input))\n",
    "train_and_save_ctgan(policy_input, 'Policy', 'Policy_ctgan.pkl')\n",
    "\n",
    "# Repeat for Coverage, Premium, Claim, Beneficiary similarly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201dd9b9-c97f-4ccf-9492-7efd4f04791f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded Customer.csv into dataframes['Customer'] with shape (5000, 6)\n",
      "âœ… Loaded Policy.csv into dataframes['Policy'] with shape (12491, 7)\n",
      "âœ… Loaded PolicyStatus.csv into dataframes['PolicyStatus'] with shape (3, 2)\n",
      "âœ… Loaded PolicyType.csv into dataframes['PolicyType'] with shape (3, 2)\n",
      "âœ… Loaded Coverage.csv into dataframes['Coverage'] with shape (31398, 4)\n",
      "âœ… Loaded CoverageType.csv into dataframes['CoverageType'] with shape (3, 2)\n",
      "âœ… Loaded Premium.csv into dataframes['Premium'] with shape (12517, 5)\n",
      "âœ… Loaded PaymentMethod.csv into dataframes['PaymentMethod'] with shape (3, 2)\n",
      "âœ… Loaded Claim.csv into dataframes['Claim'] with shape (24993, 5)\n",
      "âœ… Loaded Beneficiary.csv into dataframes['Beneficiary'] with shape (18750, 6)\n",
      "\n",
      "All loaded tables: ['Customer', 'Policy', 'PolicyStatus', 'PolicyType', 'Coverage', 'CoverageType', 'Premium', 'PaymentMethod', 'Claim', 'Beneficiary']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "csv_files = [\n",
    "    'Customer.csv',\n",
    "    'Policy.csv',\n",
    "    'PolicyStatus.csv',\n",
    "    'PolicyType.csv',\n",
    "    'Coverage.csv',\n",
    "    'CoverageType.csv',\n",
    "    'Premium.csv',\n",
    "    'PaymentMethod.csv',\n",
    "    'Claim.csv',\n",
    "    'Beneficiary.csv'\n",
    "]\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    if os.path.exists(file):\n",
    "        df_name = file.replace('.csv', '')\n",
    "        dataframes[df_name] = pd.read_csv(file)\n",
    "        print(f\"âœ… Loaded {file} into dataframes['{df_name}'] with shape {dataframes[df_name].shape}\")\n",
    "    else:\n",
    "        print(f\"âŒ File not found: {file}\")\n",
    "\n",
    "# Now you can verify all keys loaded:\n",
    "print(\"\\nAll loaded tables:\", list(dataframes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3459fcc1-0a9c-4b14-ba4c-718a16015e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_and_save_ctgan(data, table_name, model_path, epochs=200):\n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(data)\n",
    "\n",
    "    model = CTGANSynthesizer(metadata, epochs=epochs, batch_size=256, pac=1)  # pac=1 to avoid this error\n",
    "    print(f\"Starting training {table_name} model with {len(data)} rows and {epochs} epochs...\")\n",
    "    model.fit(data)\n",
    "    print(f\"{table_name} model training complete.\")\n",
    "\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Saved {table_name} model to '{model_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56a6609-89cf-4961-912a-5a7b44388b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2000147874\\Downloads\\myenv\\Lib\\site-packages\\sdv\\single_table\\base.py:162: FutureWarning:\n",
      "\n",
      "The 'SingleTableMetadata' is deprecated. Please use the new 'Metadata' class for synthesizers.\n",
      "\n",
      "C:\\Users\\2000147874\\Downloads\\myenv\\Lib\\site-packages\\sdv\\single_table\\base.py:128: UserWarning:\n",
      "\n",
      "We strongly recommend saving the metadata using 'save_to_json' for replicability in future SDV versions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training Customer model with 5000 rows and 200 epochs...\n",
      "PerformanceAlert: Using the CTGANSynthesizer on this data is not recommended. To model this data, CTGAN will generate a large number of columns.\n",
      "\n",
      "Original Column Name   Est # of Columns (CTGAN)\n",
      "date_of_birth          11\n",
      "address                5000\n",
      "\n",
      "We recommend preprocessing discrete columns that can have many values, using 'update_transformers'. Or you may drop columns that are not necessary to model. (Exit this script using ctrl-C)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_and_save_ctgan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframes\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCustomer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCustomer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCustomer_ctgan.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mtrain_and_save_ctgan\u001b[39m\u001b[34m(data, table_name, model_path, epochs)\u001b[39m\n\u001b[32m      5\u001b[39m model = CTGANSynthesizer(metadata, epochs=epochs, batch_size=\u001b[32m256\u001b[39m, pac=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# pac=1 to avoid this error\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model training complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m joblib.dump(model, model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\sdv\\single_table\\base.py:666\u001b[39m, in \u001b[36mBaseSynthesizer.fit\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    664\u001b[39m is_converted = \u001b[38;5;28mself\u001b[39m._store_and_convert_original_cols(data)\n\u001b[32m    665\u001b[39m processed_data = \u001b[38;5;28mself\u001b[39m.preprocess(data)\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_processed_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_converted:\n\u001b[32m    668\u001b[39m     data.columns = \u001b[38;5;28mself\u001b[39m._original_columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\sdv\\single_table\\base.py:635\u001b[39m, in \u001b[36mBaseSynthesizer.fit_processed_data\u001b[39m\u001b[34m(self, processed_data)\u001b[39m\n\u001b[32m    633\u001b[39m check_synthesizer_version(\u001b[38;5;28mself\u001b[39m, is_fit_method=\u001b[38;5;28;01mTrue\u001b[39;00m, compare_operator=operator.lt)\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m processed_data.empty:\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[38;5;28mself\u001b[39m._fitted = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;28mself\u001b[39m._fitted_date = datetime.datetime.today().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\sdv\\single_table\\ctgan.py:292\u001b[39m, in \u001b[36mCTGANSynthesizer._fit\u001b[39m\u001b[34m(self, processed_data)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m warnings.catch_warnings():\n\u001b[32m    291\u001b[39m     warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, message=\u001b[33m'\u001b[39m\u001b[33m.*Attempting to run cuBLAS.*\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiscrete_columns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\ctgan\\synthesizers\\base.py:50\u001b[39m, in \u001b[36mrandom_state.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.random_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     53\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m set_random_states(\u001b[38;5;28mself\u001b[39m.random_states, \u001b[38;5;28mself\u001b[39m.set_random_state):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\ctgan\\synthesizers\\ctgan.py:465\u001b[39m, in \u001b[36mCTGAN.fit\u001b[39m\u001b[34m(self, train_data, discrete_columns, epochs)\u001b[39m\n\u001b[32m    462\u001b[39m     loss_g = -torch.mean(y_fake) + cross_entropy\n\u001b[32m    464\u001b[39m     optimizerG.zero_grad(set_to_none=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[43mloss_g\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m     optimizerG.step()\n\u001b[32m    468\u001b[39m generator_loss = loss_g.detach().cpu().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Downloads\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_and_save_ctgan(dataframes['Customer'], 'Customer', 'Customer_ctgan.pkl', epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07fe2f-2053-483c-a616-a0e6bff60be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_data = dataframes['Policy'].copy()\n",
    "policy_data['customer_id'] = np.random.choice(dataframes['Customer']['customer_id'], size=len(policy_data))\n",
    "policy_data['status_id'] = np.random.choice(dataframes['PolicyStatus']['status_id'], size=len(policy_data))\n",
    "policy_data['type_id'] = np.random.choice(dataframes['PolicyType']['type_id'], size=len(policy_data))\n",
    "\n",
    "train_and_save_ctgan(dataframes['Policy'], 'Policy', 'Policy_ctgan.pkl', epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27227ba8-e44c-4df8-a877-6429a45dc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_data = dataframes['Coverage'].copy()\n",
    "coverage_data['policy_id'] = np.random.choice(dataframes['Policy']['policy_id'], size=len(coverage_data))\n",
    "coverage_data['coverage_type_id'] = np.random.choice(dataframes['CoverageType']['coverage_type_id'], size=len(coverage_data))\n",
    "\n",
    "train_and_save_ctgan(coverage_data, 'Coverage', 'Coverage_ctgan.pkl', epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3fd90-9076-4d70-9eb3-92cddbcb0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "premium_data = dataframes['Premium'].copy()\n",
    "premium_data['policy_id'] = np.random.choice(dataframes['Policy']['policy_id'], size=len(premium_data))\n",
    "premium_data['payment_method_id'] = np.random.choice(dataframes['PaymentMethod']['method_id'], size=len(premium_data))\n",
    "\n",
    "train_and_save_ctgan(premium_data, 'Premium', 'Premium_ctgan.pkl', epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ae8fd8-e719-4cee-bbbb-955897b2ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_data = dataframes['Claim'].copy()\n",
    "claim_data['policy_id'] = np.random.choice(dataframes['Policy']['policy_id'], size=len(claim_data))\n",
    "\n",
    "train_and_save_ctgan(claim_data, 'Claim', 'Claim_ctgan.pkl', epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bd7c9-cf70-45fd-9ae9-2783c4d152ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "beneficiary_data = dataframes['Beneficiary'].copy()\n",
    "beneficiary_data['policy_id'] = np.random.choice(dataframes['Policy']['policy_id'], size=len(beneficiary_data))\n",
    "\n",
    "train_and_save_ctgan(beneficiary_data, 'Beneficiary', 'Beneficiary_ctgan.pkl', epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922ec62-04d1-44c1-adee-c555770d0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== STEP 0: Install Dependencies (if not already installed) =====\n",
    "# !pip install -q sdv joblib\n",
    "\n",
    "# ===== STEP 1: Import Libraries =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ===== STEP 2: Load Original Lookup Data (for FKs) =====\n",
    "lookup_tables = {\n",
    "    'Customer': pd.read_csv('Customer.csv'),\n",
    "    'Policy': pd.read_csv('Policy.csv'),\n",
    "    'PolicyStatus': pd.read_csv('PolicyStatus.csv'),\n",
    "    'PolicyType': pd.read_csv('PolicyType.csv'),\n",
    "    'Coverage': pd.read_csv('Coverage.csv'),\n",
    "    'CoverageType': pd.read_csv('CoverageType.csv'),\n",
    "    'Premium': pd.read_csv('Premium.csv'),\n",
    "    'PaymentMethod': pd.read_csv('PaymentMethod.csv'),\n",
    "    'Claim': pd.read_csv('Claim.csv'),\n",
    "    'Beneficiary': pd.read_csv('Beneficiary.csv'),\n",
    "}\n",
    "\n",
    "# ===== STEP 3: Define how many rows to generate per table =====\n",
    "sample_counts = {\n",
    "    'Customer': 100000,      \n",
    "    'Policy': 200000,\n",
    "    'Coverage': 300000,\n",
    "    'Premium': 250000,\n",
    "    'Claim': 400000,\n",
    "    'Beneficiary': 250000\n",
    "}\n",
    "\n",
    "# ===== STEP 4: Load Models and Generate Data =====\n",
    "def load_model_and_sample(model_path, num_rows):\n",
    "    model = joblib.load(model_path)\n",
    "    synthetic = model.sample(num_rows=num_rows)\n",
    "    print(f\"âœ… Generated {num_rows} synthetic rows from model {model_path}\")\n",
    "    return synthetic\n",
    "\n",
    "# ===== STEP 5: Generate synthetic tables =====\n",
    "synthetic_tables = {}\n",
    "\n",
    "# 1. Customer (no FK)\n",
    "synthetic_tables['Customer'] = load_model_and_sample('Customer_ctgan.pkl', sample_counts['Customer'])\n",
    "\n",
    "# 2. Policy (FKs: customer_id, status_id, type_id)\n",
    "policy = load_model_and_sample('Policy_ctgan.pkl', sample_counts['Policy'])\n",
    "policy['customer_id'] = np.random.choice(synthetic_tables['Customer']['customer_id'], size=len(policy))\n",
    "policy['status_id'] = np.random.choice(lookup_tables['PolicyStatus']['status_id'], size=len(policy))\n",
    "policy['type_id'] = np.random.choice(lookup_tables['PolicyType']['type_id'], size=len(policy))\n",
    "synthetic_tables['Policy'] = policy\n",
    "\n",
    "# 3. Coverage (FKs: policy_id, coverage_type_id)\n",
    "coverage = load_model_and_sample('Coverage_ctgan.pkl', sample_counts['Coverage'])\n",
    "coverage['policy_id'] = np.random.choice(synthetic_tables['Policy']['policy_id'], size=len(coverage))\n",
    "coverage['coverage_type_id'] = np.random.choice(lookup_tables['CoverageType']['coverage_type_id'], size=len(coverage))\n",
    "synthetic_tables['Coverage'] = coverage\n",
    "\n",
    "# 4. Premium (FKs: policy_id, payment_method_id)\n",
    "premium = load_model_and_sample('Premium_ctgan.pkl', sample_counts['Premium'])\n",
    "premium['policy_id'] = np.random.choice(synthetic_tables['Policy']['policy_id'], size=len(premium))\n",
    "premium['payment_method_id'] = np.random.choice(lookup_tables['PaymentMethod']['method_id'], size=len(premium))\n",
    "synthetic_tables['Premium'] = premium\n",
    "\n",
    "# 5. Claim (FK: policy_id)\n",
    "claim = load_model_and_sample('Claim_ctgan.pkl', sample_counts['Claim'])\n",
    "claim['policy_id'] = np.random.choice(synthetic_tables['Policy']['policy_id'], size=len(claim))\n",
    "synthetic_tables['Claim'] = claim\n",
    "\n",
    "# 6. Beneficiary (FK: policy_id)\n",
    "beneficiary = load_model_and_sample('Beneficiary_ctgan.pkl', sample_counts['Beneficiary'])\n",
    "beneficiary['policy_id'] = np.random.choice(synthetic_tables['Policy']['policy_id'], size=len(beneficiary))\n",
    "synthetic_tables['Beneficiary'] = beneficiary\n",
    "\n",
    "# ===== STEP 6: Save to CSV =====\n",
    "output_folder = 'synthetic_output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for table_name, df in synthetic_tables.items():\n",
    "    file_path = os.path.join(output_folder, f\"{table_name}_synthetic.csv\")\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"ðŸ“ Saved {table_name} synthetic data to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518baa4c-22da-48cb-a8b9-47ba52dc26fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57327ff-e4c8-4627-bb60-5c7b01906a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3ab0d-44e4-407a-bfc3-05f45a382696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c28248-46b2-4971-ad39-23067edba721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= STEP 4: Load models and generate synthetic data =======\n",
    "def load_and_generate_ctgan(model_path, num_samples):\n",
    "    model = joblib.load(model_path)\n",
    "    synthetic_data = model.sample(num_samples)\n",
    "    print(f\"Generated {num_samples} rows from model '{model_path}'\")\n",
    "    return synthetic_data\n",
    "\n",
    "# Define how many samples you want per table\n",
    "samples = {\n",
    "    'Customer': 100,\n",
    "    'Policy': 200,\n",
    "    'Coverage': 200,\n",
    "    'Premium': 150,\n",
    "    'Claim': 400,\n",
    "    'Beneficiary': 300\n",
    "}\n",
    "\n",
    "# Generate synthetic Customer data (independent)\n",
    "synthetic_customer = load_and_generate_ctgan('Customer_ctgan.pkl', samples['Customer'])\n",
    "\n",
    "# Generate synthetic Policy data â€” use synthetic_customer IDs for FK consistency\n",
    "policy_data = dataframes['Policy'].copy()\n",
    "policy_data['customer_id'] = np.random.choice(synthetic_customer['customer_id'], size=len(policy_data))\n",
    "policy_data['status_id'] = np.random.choice(dataframes['PolicyStatus']['status_id'], size=len(policy_data))\n",
    "policy_data['type_id'] = np.random.choice(dataframes['PolicyType']['type_id'], size=len(policy_data))\n",
    "synthetic_policy = load_and_generate_ctgan('Policy_ctgan.pkl', samples['Policy'])\n",
    "\n",
    "# Generate synthetic Coverage data â€” use synthetic_policy IDs\n",
    "coverage_data = dataframes['Coverage'].copy()\n",
    "coverage_data['policy_id'] = np.random.choice(synthetic_policy['policy_id'], size=len(coverage_data))\n",
    "coverage_data['coverage_type_id'] = np.random.choice(dataframes['CoverageType']['coverage_type_id'], size=len(coverage_data))\n",
    "synthetic_coverage = load_and_generate_ctgan('Coverage_ctgan.pkl', samples['Coverage'])\n",
    "\n",
    "# Generate synthetic Premium data â€” use synthetic_policy IDs\n",
    "premium_data = dataframes['Premium'].copy()\n",
    "premium_data['policy_id'] = np.random.choice(synthetic_policy['policy_id'], size=len(premium_data))\n",
    "premium_data['payment_method_id'] = np.random.choice(dataframes['PaymentMethod']['payment_method_id'], size=len(premium_data))\n",
    "synthetic_premium = load_and_generate_ctgan('Premium_ctgan.pkl', samples['Premium'])\n",
    "\n",
    "# Generate synthetic Claim data â€” use synthetic_policy IDs\n",
    "claim_data = dataframes['Claim'].copy()\n",
    "claim_data['policy_id'] = np.random.choice(synthetic_policy['policy_id'], size=len(claim_data))\n",
    "synthetic_claim = load_and_generate_ctgan('Claim_ctgan.pkl', samples['Claim'])\n",
    "# Generate synthetic Beneficiary data â€” use synthetic_policy IDs\n",
    "beneficiary_data = dataframes['Beneficiary'].copy()\n",
    "beneficiary_data['policy_id'] = np.random.choice(synthetic_policy['policy_id'], size=len(beneficiary_data))\n",
    "synthetic_beneficiary = load_and_generate_ctgan('Beneficiary_ctgan.pkl', samples['Beneficiary'])\n",
    "\n",
    "# Optional: Export synthetic data to CSV files\n",
    "synthetic_customer.to_csv('synthetic_customer.csv', index=False)\n",
    "synthetic_policy.to_csv('synthetic_policy.csv', index=False)\n",
    "synthetic_coverage.to_csv('synthetic_coverage.csv', index=False)\n",
    "synthetic_premium.to_csv('synthetic_premium.csv', index=False)\n",
    "synthetic_claim.to_csv('synthetic_claim.csv', index=False)\n",
    "synthetic_beneficiary.to_csv('synthetic_beneficiary.csv', index=False)\n",
    "\n",
    "print(\"Synthetic data generated and saved to CSV files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2abebd-453b-4829-b907-ed4322989486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90dae44-4541-4b7f-8f66-eb2c07a95f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88371b-f13b-4b41-99bb-936e6648d90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sdv.metadata import Metadata\n",
    "from sdv.lite import SingleTablePreset\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9304067f-5b1e-4dfc-9489-3f4afe824abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "df = pd.read_csv(\"Customer.csv\")\n",
    "\n",
    "# Clean column names (remove extra spaces, BOMs)\n",
    "df.columns = df.columns.str.strip().str.replace('\\ufeff', '')\n",
    "\n",
    "# Drop rows with any nulls\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert customer_id to integer (if it's float-like but actually int)\n",
    "if df['customer_id'].dtype == 'float':\n",
    "    df['customer_id'] = df['customer_id'].astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15679b-4d6c-43af-af40-4a51ce6be580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata automatically from the DataFrame\n",
    "metadata = Metadata()\n",
    "metadata.detect_from_dataframe(data=df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

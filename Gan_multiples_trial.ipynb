{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i2_6IkMqh0Ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install needed packages\n",
        "!pip install pandas numpy tensorflow faker\n",
        "\n",
        "# Step 2: Import\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Step 3: Load your CSV file\n",
        "df = pd.read_csv('/content/Customer.csv')  # Replace with your uploaded file\n",
        "\n",
        "# Step 4: Convert DOB to age\n",
        "df['date_of_birth'] = pd.to_datetime(df['date_of_birth'], errors='coerce')\n",
        "today = pd.to_datetime('today')\n",
        "df['age'] = (today.year - df['date_of_birth'].dt.year).fillna(30).astype(int)\n",
        "\n",
        "# Step 5: Normalize the age values between -1 and 1\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "age_scaled = scaler.fit_transform(df[['age']])\n",
        "\n",
        "# Step 6: Define the Generator\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(10,)),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='tanh')  # Output a single value\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Step 7: Define the Discriminator\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(1,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')  # 1 for real, 0 for fake\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Step 8: Instantiate the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Step 9: Compile the Discriminator\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Step 10: Build and Compile the GAN\n",
        "discriminator.trainable = False\n",
        "gan_input = tf.keras.Input(shape=(10,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = tf.keras.Model(gan_input, gan_output)\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrIyD4SypIZT",
        "outputId": "c6991ca7-0bed-4b9a-fd66-539a1fd40d21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting faker\n",
            "  Downloading faker-37.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading faker-37.3.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-5be63f9004a3>:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['date_of_birth'] = pd.to_datetime(df['date_of_birth'], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild Generator\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(10,)),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(1, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Rebuild Discriminator\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(1,)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(16, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Re-create the models\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "# Compile the discriminator first\n",
        "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Freeze discriminator\n",
        "discriminator.trainable = False\n",
        "\n",
        "# Build GAN model\n",
        "gan_input = tf.keras.Input(shape=(10,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = tf.keras.Model(gan_input, gan_output)\n",
        "\n",
        "# Compile GAN\n",
        "gan.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "metadata": {
        "id": "TIIL9DItpT9w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Assume generator is your trained Keras model from before\n",
        "num_samples = 1000\n",
        "noise_dim = 10  # match your generator input\n",
        "\n",
        "noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
        "generated_scaled_ages = generator.predict(noise)\n",
        "\n",
        "synthetic_ages = scaler.inverse_transform(generated_scaled_ages)\n",
        "synthetic_ages = synthetic_ages.flatten().astype(int)\n",
        "\n",
        "print(synthetic_ages[:10])\n",
        "\n",
        "# Inverse transform to original age scale\n",
        "# Assuming you used MinMaxScaler fitted on real ages earlier as 'scaler'\n",
        "synthetic_ages = scaler.inverse_transform(generated_scaled_ages)\n",
        "\n",
        "# Convert to integer ages\n",
        "synthetic_ages = synthetic_ages.flatten().astype(int)\n",
        "\n",
        "print(synthetic_ages[:10])  # Check first 10 synthetic ages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d1zYYVwp5QL",
        "outputId": "edc26cec-8e6f-4280-9b20-ab1497771951"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "[30 30 30 30 30 30 30 30 30 30]\n",
            "[30 30 30 30 30 30 30 30 30 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Train the GAN\n",
        "epochs = 5000\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Real samples\n",
        "    idx = np.random.randint(0, age_scaled.shape[0], batch_size)\n",
        "    real_ages = age_scaled[idx]\n",
        "\n",
        "    # Fake samples\n",
        "    noise = np.random.normal(0, 1, (batch_size, 10))\n",
        "    fake_ages = generator.predict(noise, verbose=0)\n",
        "\n",
        "    # Train discriminator\n",
        "    d_loss_real = discriminator.train_on_batch(real_ages, np.ones((batch_size, 1)))\n",
        "    d_loss_fake = discriminator.train_on_batch(fake_ages, np.zeros((batch_size, 1)))\n",
        "\n",
        "    # Train generator\n",
        "    noise = np.random.normal(0, 1, (batch_size, 10))\n",
        "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
        "\n",
        "    # Print progress\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"{epoch} [D loss: {(d_loss_real[0] + d_loss_fake[0]):.4f}] [G loss: {g_loss:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnnYhp93pdrb",
        "outputId": "4271d930-b5ec-464b-96b5-a14da7206184"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 [D loss: 1.3089] [G loss: 0.6188]\n",
            "100 [D loss: 1.3398] [G loss: 0.5936]\n",
            "200 [D loss: 1.3480] [G loss: 0.5870]\n",
            "300 [D loss: 1.3518] [G loss: 0.5841]\n",
            "400 [D loss: 1.3539] [G loss: 0.5824]\n",
            "500 [D loss: 1.3552] [G loss: 0.5813]\n",
            "600 [D loss: 1.3562] [G loss: 0.5805]\n",
            "700 [D loss: 1.3569] [G loss: 0.5800]\n",
            "800 [D loss: 1.3574] [G loss: 0.5795]\n",
            "900 [D loss: 1.3579] [G loss: 0.5792]\n",
            "1000 [D loss: 1.3582] [G loss: 0.5789]\n",
            "1100 [D loss: 1.3585] [G loss: 0.5787]\n",
            "1200 [D loss: 1.3587] [G loss: 0.5785]\n",
            "1300 [D loss: 1.3589] [G loss: 0.5783]\n",
            "1400 [D loss: 1.3591] [G loss: 0.5782]\n",
            "1500 [D loss: 1.3593] [G loss: 0.5781]\n",
            "1600 [D loss: 1.3594] [G loss: 0.5780]\n",
            "1700 [D loss: 1.3595] [G loss: 0.5779]\n",
            "1800 [D loss: 1.3596] [G loss: 0.5778]\n",
            "1900 [D loss: 1.3597] [G loss: 0.5777]\n",
            "2000 [D loss: 1.3598] [G loss: 0.5776]\n",
            "2100 [D loss: 1.3599] [G loss: 0.5776]\n",
            "2200 [D loss: 1.3600] [G loss: 0.5775]\n",
            "2300 [D loss: 1.3600] [G loss: 0.5775]\n",
            "2400 [D loss: 1.3601] [G loss: 0.5774]\n",
            "2500 [D loss: 1.3602] [G loss: 0.5774]\n",
            "2600 [D loss: 1.3602] [G loss: 0.5773]\n",
            "2700 [D loss: 1.3603] [G loss: 0.5773]\n",
            "2800 [D loss: 1.3603] [G loss: 0.5773]\n",
            "2900 [D loss: 1.3604] [G loss: 0.5772]\n",
            "3000 [D loss: 1.3604] [G loss: 0.5772]\n",
            "3100 [D loss: 1.3604] [G loss: 0.5772]\n",
            "3200 [D loss: 1.3605] [G loss: 0.5771]\n",
            "3300 [D loss: 1.3605] [G loss: 0.5771]\n",
            "3400 [D loss: 1.3606] [G loss: 0.5771]\n",
            "3500 [D loss: 1.3606] [G loss: 0.5771]\n",
            "3600 [D loss: 1.3606] [G loss: 0.5771]\n",
            "3700 [D loss: 1.3607] [G loss: 0.5770]\n",
            "3800 [D loss: 1.3607] [G loss: 0.5770]\n",
            "3900 [D loss: 1.3607] [G loss: 0.5770]\n",
            "4000 [D loss: 1.3607] [G loss: 0.5770]\n",
            "4100 [D loss: 1.3608] [G loss: 0.5770]\n",
            "4200 [D loss: 1.3608] [G loss: 0.5770]\n",
            "4300 [D loss: 1.3608] [G loss: 0.5769]\n",
            "4400 [D loss: 1.3608] [G loss: 0.5769]\n",
            "4500 [D loss: 1.3609] [G loss: 0.5769]\n",
            "4600 [D loss: 1.3609] [G loss: 0.5769]\n",
            "4700 [D loss: 1.3609] [G loss: 0.5769]\n",
            "4800 [D loss: 1.3609] [G loss: 0.5769]\n",
            "4900 [D loss: 1.3609] [G loss: 0.5769]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ctgan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndt9O_sfsKy0",
        "outputId": "cca4bca5-0577-48f9-cc8b-6d853964b6d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ctgan in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.23.3 in /usr/local/lib/python3.11/dist-packages (from ctgan) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (2.2.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm<5,>=4.29 in /usr/local/lib/python3.11/dist-packages (from ctgan) (4.67.1)\n",
            "Requirement already satisfied: rdt>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from ctgan) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ctgan) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ctgan) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.0->ctgan) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (1.6.1)\n",
            "Requirement already satisfied: Faker>=17 in /usr/local/lib/python3.11/dist-packages (from rdt>=1.14.0->ctgan) (37.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->ctgan) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->ctgan) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->ctgan) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->ctgan) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.3->rdt>=1.14.0->ctgan) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->ctgan) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from ctgan import CTGAN\n",
        "# Load your real dataset\n",
        "real_data = pd.read_csv(\"Customer.csv\")\n",
        "\n",
        "# Drop unique ID for training\n",
        "real_data = real_data.drop(columns=[\"customer_id\"])\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_columns = [\"first_name\", \"last_name\", \"date_of_birth\", \"address\", \"phone_number\"]\n",
        "\n",
        "# Initialize CTGAN model\n",
        "ctgan = CTGAN(epochs=300)  # train for 300 epochs\n",
        "\n",
        "# Train the model\n",
        "ctgan.fit(real_data, discrete_columns=categorical_columns)\n",
        "\n",
        "# Generate synthetic data samples\n",
        "synthetic_data = ctgan.sample(500)\n",
        "\n",
        "# Add synthetic UUIDs for customer_id\n",
        "import uuid\n",
        "synthetic_data[\"customer_id\"] = [str(uuid.uuid4()) for _ in range(len(synthetic_data))]\n",
        "\n",
        "# Reorder columns\n",
        "cols = [\"customer_id\"] + [col for col in synthetic_data.columns if col != \"customer_id\"]\n",
        "synthetic_data = synthetic_data[cols]\n",
        "\n",
        "print(synthetic_data.head())\n",
        "\n",
        "synthetic_data.to_csv(\"synthetic_CTGan_customers.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gevBJL-tsJCN",
        "outputId": "53207932-38f0-4bf7-acf1-b714bb5583a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            customer_id first_name last_name date_of_birth  \\\n",
            "0  12736124-0ff1-4c46-8ffc-b8b797afef20     Smythe   Casarez    10/17/1971   \n",
            "1  42df858e-a5b1-40d7-9c76-67d63ff49661    Killian     Shine     6/17/1971   \n",
            "2  dcc5122b-22c5-4fea-9972-4acc7d459de8    Salazar  Trinidad    11/10/1971   \n",
            "3  9b38282f-dfda-4c8d-bbab-882de055e8e6    Varnado    Urbano      6/1/1971   \n",
            "4  4a7dd2a9-3a61-4107-8f91-fcbc4840e873  Cardinale      Maya    10/17/1971   \n",
            "\n",
            "      address    phone_number  \n",
            "0    Sec-1998  (557) 557-7957  \n",
            "1    Sec-1158  (713) 413-4513  \n",
            "2      B-1681  (750) 450-5150  \n",
            "3      D-1244  (620) 620-9220  \n",
            "4  Block-1418  (656) 256-7456  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T9RpzPXFOlpd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}